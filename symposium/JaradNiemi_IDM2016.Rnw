\documentclass[handout]{beamer}

\usetheme{AnnArbor}
\usecolortheme{beaver}

\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{enumerate items}[default]
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}
\setbeamertemplate{enumerate subsubitem}{\roman{enumiii}.}
\setkeys{Gin}{width=0.6\textwidth}

\title[Forecasting from low counts]{Stochastic dynamic models for low count observations}
\subtitle{(and forecasting from them)}
\author[Jarad Niemi]{Dr. Jarad Niemi}
\institute[ISU]{Iowa State University}
\date{\today}

\newcommand{\ind}{\stackrel{ind}{\sim}}

\begin{document}

<<options, results='hide', echo=FALSE, purl=FALSE>>=
opts_chunk$set(comment=NA, 
               fig.width=7, fig.height=5, 
               size='tiny', 
               out.width='0.8\\textwidth', 
               fig.align='center', 
               message=FALSE,
               echo=FALSE,
               cache=TRUE)
options(width=120)
@

<<libraries, message=FALSE, warning=FALSE, cache=FALSE>>=
library(plyr)
library(dplyr)
library(tidyr)
library(ggplot2)
library(tlpl)
source("tlpl_predict.r") # Temporary until the function gets fixed in the tlpl package
@

<<colors>>=
stateColors <- c('seagreen','red','blue')
names(stateColors) <- c('S','I','R')
colState <- scale_colour_manual(name = "state", values = stateColors)
fillState = scale_fill_manual(name = 'state', values = stateColors)

transitionColors = c('red','blue')
names(transitionColors) = c('SI','IR')
colTransition = scale_colour_manual(name = 'transition', values = transitionColors)
fillTransition = scale_fill_manual(name = 'transition', values = transitionColors)
@

<<set_seed>>=
set.seed(1)
@

\frame{\maketitle}

% \begin{frame}
% \frametitle{Outline}
% 
% \begin{itemize}
% \item 
% \end{itemize}
% \end{frame}

\begin{frame}{Overview}
\tableofcontents
\end{frame}

\section{Poisson-binomial state-space model}
\begin{frame}
\frametitle{S$\to$I$\to$R stochastic compartment model}\small
	Focus on Susceptible (S) - Infectious (I) - Recovered (R) model with Poisson transitions, i.e. 
	\[
	\overrightarrow{SI} \sim Po(\lambda_{\mbox{S}\to\mbox{I}} \mbox{S}\mbox{I} / \mbox{N} ) \qquad \overrightarrow{IR} \sim Po(\lambda_{\mbox{I}\to\mbox{R}} \mbox{I})
	\]
	\pause
	updating the states, i.e. 
	\[
	S_{t+1} = S_t - \overrightarrow{SI}, \quad 
	I_{t+1} = I_t + \overrightarrow{SI} - \overrightarrow{IR}, \quad 
	R_{t+1} = R_t + \overrightarrow{IR}
	\]
	\pause
	and binomial obserations
	\[ Y_{\mbox{S}\to\mbox{I}} \sim Bin(\overrightarrow{SI}, \theta_{\mbox{S}\to\mbox{I}}) \qquad Y_{\mbox{I}\to\mbox{R}} \sim Bin(\overrightarrow{IR}, \theta_{\mbox{I}\to\mbox{R}}) \]
	\pause
	A more general stochastic dynamic modeling structure can be used to extended to geographical regions, subpopulations, etc. 
\end{frame}



% \begin{frame}
% \frametitle{Poisson-binomial state-space model}
% Let $X_{tk}$ be the count of the number of individuals in state $k$ and time $t$. We model state transitions using Poisson distributions, i.e. \[ 
% \begin{array}{rll}
% 	\Delta X_{tk} &\stackrel{ind}{\sim} Po(\lambda_{k} f_k(X_{t-1})), \\
% 	X_{tm} &= X_{t-1,m} + \sum_{k=1}^K v_{mk} \Delta X_{tk}, & m=1,\ldots,M 
% \end{array}
% \]
% where $f_k(\cdot)$ are known functions and $v_{mk}$ is a known \emph{stoichiometry}. 
% 
% \vspace{0.2in} \pause
% 
% We assume the transitions $\Delta X_{tk}$ are partially observed through a binomial distribution, \pause i.e. 
% \[ 
% Y_{tk} \stackrel{ind}{\sim} Bin(\Delta X_{tk},\theta_{k}).
% \]
% \end{frame}

%\subsection{Simulations}
\begin{frame}
\frametitle{SIR modeling simulations}

<<state_sim>>=
sir = sckm('sir')
n = 52
d = tau_leap(sir, n)
d$X = as.data.frame(d$X)
names(d$X) = sir$states
d$X$time = 0:n
X = gather(d$X, state, count, -time)
@

<<state_sims_plot, dependson=c('state_sim','colors')>>=
ggplot(X, aes(time,count,color=state,linetype=state)) + 
  geom_line(size=2) + 
  colState
@
\end{frame}




\begin{frame}
\frametitle{SIR modeling simulations}

<<state_sims>>=
sir = sckm('sir')
n = 52

d = rdply(1000, {
  d = tau_leap(sir, n)
  d$X = as.data.frame(d$X)
  names(d$X) = sir$states
  d$X$time = 0:n
  gather(d$X, state, count, -time)
})

q = d %>% 
  group_by(time,state) %>%
  summarise(L = quantile(count,.025),
            U = quantile(count,.975))
@

<<dependson=c('state_sim','colors')>>=
ggplot(q, aes(time,ymin=L,ymax=U,fill=state)) +
  geom_ribbon(alpha=0.5) +
  fillState
@
\end{frame}


%' \begin{frame}
%' \frametitle{Variability as a function of population size}
%' 
%' <<state_sim_population>>=
%' sir = sckm('sir')
%' n = 52
%' 
%' 
%' d = ddply(data.frame(N=10^(2:4)), .(N), function(x) {
%'   sir = sckm('sir')
%'   sir$X[1] = x$N*.99
%'   sir$X[2] = x$N*.01
%'   sir$mult[1] = 1/x$N
%'   
%'   rdply(1e3, {
%'     d = tau_leap(sir, n)
%'     d$X = as.data.frame(d$X)
%'     names(d$X) = sir$states
%'     d$X$time = 0:n
%'     gather(d$X, state, count, -time)
%'   })
%' })
%' 
%' q = d %>% 
%'   group_by(time,state,N) %>%
%'   summarise(L = quantile(count,.025),
%'             U = quantile(count,.975)) %>%
%'   mutate(Lp = L/N,
%'          Up = U/N)
%' @
%' 
%' <<dependson=c('state_sim_population','colors')>>=
%' ggplot(q, aes(time,ymin=L,ymax=U,fill=state)) +
%'   geom_ribbon(alpha=0.5) +
%'   facet_wrap(~N) + 
%'   fillState
%' @
%' 
%' \end{frame}
%' 
%' 
%' \begin{frame}
%' \frametitle{Forecasts for proportion of population}
%' <<dependson=c('state_sim_population','colors')>>=
%' ggplot(q, aes(time,ymin=Lp,ymax=Up,fill=state)) +
%'   geom_ribbon(alpha=0.5) +
%'   facet_wrap(~N) +
%'   fillState
%' @
%' \end{frame}







\section{Inference and forecasting}
\begin{frame}
\frametitle{Forecasting with perfect information}

Suppose you know transition rates $\lambda$, observation probabilities $\theta=1$, and the states $X_{0:t}$ and your only goal is to forecast the future states $X_{t+1:T}$, \pause i.e. 

\[ p(X_{t+1:T}|\theta,\lambda,X_{0:t})=p(X_{t+1:T}|\theta,\lambda,X_{t}) \]
\pause 
this distribution is estimated via Monte Carlo simulation.
\end{frame}


\begin{frame}
<<perfect_forecasts>>=
# Initial simulation
sir = sckm('sir')
n = 51
d = tau_leap(sir, n)
d$X = as.data.frame(d$X)
names(d$X) = sir$states
d$X$time = 0:n
d$X = d$X %>%
  filter((time %% 10) == 0) %>%
  mutate(start_time = time)
#X = gather(d$X, state, count, -time)

# For each time-point forward simulate to the end
# and then calculate quantiles
dd = ddply(d$X, .(start_time), function(x) {
  rdply(1e3, {
    sir = sckm('sir')
    n = 52-x$time
    sir$X = x[,c("S","I","R")]
    d = tau_leap(sir, n)
    d$X = as.data.frame(d$X)
    names(d$X) = sir$states
    d$X$time = x$start_time:52
    d$X %>%
      gather(state, count, -time)
  })
})

q = dd %>% 
  group_by(start_time,time,state) %>%
  summarise(L = quantile(count,.025),
            U = quantile(count,.975)) 
@

<<dependson=c('perfect_forecasts','colState')>>=
q$start_time = paste("t=", q$start_time)
ggplot(q, aes(time, ymin=L, ymax=U, fill=state)) +
  geom_ribbon(alpha=0.5) +
  facet_wrap(~start_time) +
  fillState
@
\end{frame}


\begin{frame}
\frametitle{Delay in data analysis}

Suppose you have a one or two week delay in collecting, processing, and analyzing so that when trying to forecast future states you are using ``old'' data, \pause i.e.

\[ 
p(X_{t+1:T}|\theta,\lambda,X_{t-L})
\]

\pause 
where 
\begin{itemize}
\item $L=0$ indicates up-to-date data
\item $L=1$ indicates one-week old data
\item $L=2$ indicates two-week old data
\end{itemize}
\end{frame}



\begin{frame}
\frametitle{}
<<delayed_data>>=
# Initial simulation
sir = sckm('sir')
n = 52
d = tau_leap(sir, n)
d$X = as.data.frame(d$X)
names(d$X) = sir$states
d$X$time = 0:n
d$X = d$X 

# For each time-point forward simulate to the end
# and then calculate quantiles
dd = ddply(d$X, .(time), function(x) {
  rdply(1e3, {
    sir = sckm('sir')
    sir$X = x[,c("S","I","R")]
    d = tau_leap(sir, 3)
    d$X = as.data.frame(d$X)
    names(d$X) = sir$states
#    d$X$time = x$start_time + 0:3
    d$X$delay = c(NA,0:2)
    d$X  %>% 
      na.omit %>%
      gather(state, count, -delay)
  })
})

q = dd %>%
  group_by(time,delay,state) %>%
  summarise(L = quantile(count,.025),
            U = quantile(count,.975)) 
@

<<dependson=c('perfect_forecasts','colState')>>=
q = q %>%
  mutate(delay_formatted = paste("L=",delay))
ggplot(q, aes(time, ymin=L, ymax=U, fill=state)) +
  geom_ribbon(alpha=0.5) + 
  facet_wrap(~delay_formatted) + 
  fillState
@
\end{frame}

\begin{frame}
<<dependson=c('perfect_forecasts','colState')>>=
q = q %>%
  mutate(delay_formatted = paste("L=",delay))
ggplot(q %>% filter(state=='I'), aes(time, ymin=L, ymax=U, fill=state)) +
  geom_ribbon(alpha=0.5) + 
  facet_wrap(~delay_formatted) +
  fillState
@
\end{frame}



\section{Forecasting with noisy observations}
\begin{frame}
\frametitle{Forecasting with noisy observations}

Suppose, we know the transition rates ($\lambda$) and the observation probabilities ($\theta$), but we only observe a noisy version of the state transitions, i.e. . 
\[ Y_{\mbox{S}\to\mbox{I}} \sim Bin(\overrightarrow{SI}, \theta_{\mbox{S}\to\mbox{I}}) \qquad Y_{\mbox{I}\to\mbox{R}} \sim Bin(\overrightarrow{IR}, \theta_{\mbox{I}\to\mbox{R}}) \]

\vspace{0.2in} \pause

Now the forecast distribution we need is 
\[
p(X_{t+1:T}|\lambda,\theta, y_{0:t}) = \int p(X_{t+1:T},\lambda,\theta|X_{t})p(X_{t}|\lambda,\theta, y_{0:t}) dX_{t}.
\]
\end{frame}



% \begin{frame}
% \frametitle{Noisy observation}
% 
% Recall, we model state transitions using Poisson distributions, i.e. \[ 
% \begin{array}{rll}
% 	\Delta X_{tk} &\stackrel{ind}{\sim} Po(\lambda_{k} f_k(X_{t-1})), \\
% 	X_{tm} &= X_{t-1,m} + \sum_{k=1}^K v_{mk} \Delta X_{tk}, & m=1,\ldots,M 
% \end{array}
% \]
% where $f_k(\cdot)$ are known functions and $v_{mk}$ is a known \emph{stoichiometry}. 
% 
% \vspace{0.2in} 
% 
% We assume the transitions $\Delta X_{tk}$ are partially observed through a binomial distribution, i.e. 
% \[ 
% Y_{tk} \stackrel{ind}{\sim} Bin(\Delta X_{tk},\theta_{k}).
% \]
% \end{frame}




\begin{frame}
<<noisy_data>>=
set.seed(2)
sir = sckm('sir')
n = 52
d = tau_leap(sir, n)
d$nr = as.data.frame(d$nr)
names(d$nr) = c('SI','IR')
d$nr$time = 1:n 

dd = ddply(expand.grid(p_SI = 10^-c(Inf,3:1),
                      p_IR = 10^-c(Inf,3:1)),
          .(p_SI,p_IR), 
          function(x,df) {
  data.frame(SI = rbinom(nrow(df), df$SI, x$p_SI),
             IR = rbinom(nrow(df), df$IR, x$p_IR),
             time=df$time) 
}, df=d$nr) 
@

<<noisy_data_plot, dependson=c('noisy_data','colors')>>=
dd$p_SI_formatted = paste('S to I:', dd$p_SI)
dd$p_IR_formatted = paste('I to R:', dd$p_IR)
ggplot(dd %>%
         select(SI, IR, time, p_SI_formatted, p_IR_formatted) %>% 
         gather(transition, count, -time, -p_SI_formatted, -p_IR_formatted), 
       aes(time,count,color=transition,shape=transition)) + 
  geom_point() +
  facet_grid(p_IR_formatted~p_SI_formatted) +
  colTransition
@
\end{frame}


\begin{frame}
\frametitle{Noisily observed state}
<<noisy_data_forecasts,dependson='noisy_data'>>=
sir = sckm('sir')
tp = 20

pred = ddply(dd %>% filter(time<tp), .(p_SI,p_IR), function(x) {
  nn = 10000 # to make prior informative
  pp = c(x$p_SI[1], x$p_IR[1])
  prior_  = list(rate = list(a = nn*sir$theta, b = nn),
                 prob = list(a = nn*pp, b = nn*(1-pp)),
                 X = sir$X)

  tlpl = tlpl(list(y=x %>% select(SI,IR), tau=1),
            sckm = sir,
            prior = prior_,
            n.particles = 10000)
  pX = tlpl_predict(sir, 52-tp, tlpl, verbose=0)$X
  q = adply(pX, c(1,3), function(x) {
    data.frame(L = quantile(x, .025),
               U = quantile(x,.975))
  })
  names(q)[1:2] = c('state','time')
  q$state = mapvalues(q$state, 1:3, sir$states)
  q$time = as.numeric(q$time)+tp
  q
})
@

<<noisy_data_forecasts_plot, dependson=c('noisy_data_forecasts','colors')>>=
pred = pred %>%
  mutate(p_SI_formatted = paste('S to I:', p_SI),
         p_IR_formatted = paste('I to R:', p_IR))

ggplot(pred, aes(time, ymin=L, ymax=U, fill=state)) +
  geom_ribbon(alpha=0.5) + 
  facet_grid(p_IR_formatted~p_SI_formatted) +
  fillState
@
\end{frame}


\section{Forecasting with inference on parameters}
\begin{frame}
\frametitle{Forecasting with inference on parameters}

In reality, we don't know the transition rates ($\lambda$) and the observation probabilities ($\theta$), and we only observe a noisy version of the state transitions.

\vspace{0.2in} \pause

Now the forecast distribution we need is 
\[
p(X_{t+1:T}|y_{0:t}) = \int \int \int p(X_{t+1:T}|\lambda,\theta,X_{t})p(X_{t},\lambda,\theta|y_{0:t}) d\lambda d\theta dX_{t}.
\]
\end{frame}


\begin{frame}
\frametitle{Prior distributions}

In order to calculate (or approximate) the integral
\[
\int \int \int p(X_{t+1:T},\lambda,\theta|X_{t})p(X_{t},\lambda,\theta|y_{0:t}) d\lambda d\theta dX_{t}
\]
we need to assign priors to $\lambda$, $\theta$, and $X_0$. \pause 
Suppose, we assume 
\[ \begin{array}{rl}
\theta_k &\ind Be(n_\theta p_k, n_\theta [1-p_k]) \\
\lambda_k &\ind Ga(n_\lambda c_k,n_\lambda ) \\
X_0 &\sim Mult(N;z_1,\ldots,z_S) 
\end{array} \]
\pause
We can control how informative the priors are with $n_\theta$ and $n_\lambda$. 
\end{frame}






<<inference_forecasts,dependson='noisy_data'>>=
sir = sckm('sir')
tp = 20

pred = ddply(dd %>% filter(time<tp), .(p_SI,p_IR), function(x) {
  ddply(expand.grid(n_theta = 10^(c(0,2,4)),
                    n_lambda = 10^(c(0,2,4))),
        .(n_theta,n_lambda),
        function(xx) {
          pp = c(x$p_SI[1], x$p_IR[1])
          prior_  = list(rate = list(a = xx$n_lambda*sir$theta, b = xx$n_lambda),
                         prob = list(a = xx$n_theta*pp, b = xx$n_theta*(1-pp)),
                         X = sir$X)
          
          tlpl = tlpl(list(y = x %>% select(SI,IR), tau=1),
                      sckm = sir,
                      prior = prior_,
                      n.particles = 10000)
          pX = tlpl_predict(sir, 52-tp, tlpl, verbose=0)$X
          q = adply(pX, c(1,3), function(x) {
            data.frame(L = quantile(x, .025),
                       U = quantile(x, .975))
          })
          names(q)[1:2] = c('state','time')
          q$state = mapvalues(q$state, 1:3, sir$states)
          q$time  = as.numeric(q$time)+tp
          q
        })
})
@

<<inference_forecasts_data, dependson='inference_forecasts'>>=
pred = pred %>%
  mutate(p_SI_formatted = paste('S to I:', p_SI),
         p_IR_formatted = paste('I to R:', p_IR),
         n_theta_formatted = paste('n_theta:', n_theta),
         n_lambda_formatted = paste('n_lambda:', n_lambda))
@

\begin{frame}
\frametitle{Informative priors}
<<informative_priors, dependson=c('inference_forecasts_data','colors')>>=
ggplot(pred %>% filter(p_SI == .1, p_IR == .1), 
       aes(time, ymin=L, ymax=U, fill=state)) +
  geom_ribbon(alpha=0.5) + 
  facet_grid(n_theta_formatted~n_lambda_formatted) +
  fillState
@
\end{frame}

\begin{frame}
\frametitle{Balance priors and data}
<<balance_priors_data, dependson=c('inference_forecasts_data','colors')>>=
ggplot(pred %>% filter(p_IR == 0.01, n_theta==100), 
       aes(time, ymin=L, ymax=U, fill=state)) +
  geom_ribbon(alpha=0.5) + 
  facet_grid(p_SI_formatted~n_lambda_formatted) +
  fillState
@
\end{frame}


\begin{frame}
\frametitle{We need information}

Information can come from 
\begin{itemize}
\item Data
\item Priors
\end{itemize}

\vspace{0.2in} \pause

We can quantitatively assess the impact of better information, \pause i.e. 
\begin{itemize}[<+->]
\item increasing prior information, e.g. $\lambda\sim Ga(n_\lambda c_k, n_\lambda)$ by increasing $n_\lambda$,
\item increasing surveillance, e.g.  $Y \sim Bin(S\to I, \theta)$ by increasing $\theta$, and
\item increasing timeliness, e.g. $p(X_{t+1:T}|y_{1:t-L})$ by decreasing $L$.
\end{itemize}
\pause
Then, we can discuss how to assign resources depending on the costs associated with each impact above.

\end{frame}

\end{document}
