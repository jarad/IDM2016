\documentclass{beamer}

\usetheme{AnnArbor}\usecolortheme{beaver}

\usepackage{verbatim,xmpmulti,color,multicol,multirow,hyperref,fancyvrb}
\setlength{\unitlength}{\textwidth}  % measure in textwidths
\usepackage[normalem]{ulem}

\setbeamertemplate{navigation symbols}{}
\setkeys{Gin}{width=0.7\textwidth}

\usepackage{tikz}
\usetikzlibrary{fit,positioning,calc}

\graphicspath{{figures/}}

\setbeamertemplate{enumerate item}{\insertenumlabel. }
\setbeamertemplate{enumerate subitem}{\alph{enumii}.}


\title[Particle learning]{Particle learning for low counts in disease outbreaks}
\author{Jarad Niemi\\with Mike Ludkovski and Nicholas Michaud}
\institute[Iowa State]{Iowa State University}
\date{\today}

\newcommand{\iid}{\stackrel{iid}{\sim}}
\newcommand{\ind}{\stackrel{ind}{\sim}}
\newcommand{\I}{\mathrm{I}}
\newcommand{\Yiid}{Y_1,\ldots,Y_n\stackrel{iid}{\sim}}

\begin{document}

%\section{Temp??} \begin{comment}

\frame{\titlepage}

\section{Outline}
\frame{\frametitle{Outline}
	\begin{itemize}
	\item Measles outbreak in Zimbabwe
	\item Model for low counts in disease outbreaks
	\item Particle learning
	\item Simulation study
	\item Application to outbreak in Zimbabwe
	\end{itemize}
}

\section{Making decisions based on surveillance data}

\begin{frame}
\frametitle{Making decisions based on surveillance data}
\pause

The primary purpose of this work is to 
\begin{quote}
use surveillance data to help inform public health officials on control measures.
\end{quote}

\vspace{0.1in} \pause 

Measles outbreak in Zimbabwe (2009-2010):

\begin{itemize}[<+->]
\item Late summer of 2009, measles detected in Zimbabwe
\item Reporting of measles added to regular cholera reporting

\begin{quote}
Lab confirmed: Suspected case of measles with positive serum IgM antibody, with no history of measles vaccination in the past 4 weeks.
\end{quote}

\item Fall of 2009, localized vaccination campaign
\item Measles spread across the country
\item Summer 2010, mass vaccination campaign
\item Fall 2010, no additional measles cases reported
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Measles outbreak in Zimbabwe (2009-2010)}
\setkeys{Gin}{width=0.6\textwidth}

	\begin{center}
	\only<1-| handout:0>{\multiinclude[<+>][format=pdf]{total-cases}}
    	\only<beamer:0| handout:1>{\includegraphics{total-cases-7}}
	\end{center}
\end{frame}

\frame{\frametitle{Outline}
	\begin{itemize}
	\item Measles outbreak in Zimbabwe
	\item \alert{Model for low counts in disease outbreaks}
	\item Particle learning
	\item Simulation study
	\item Application to outbreak in Zimbabwe
	\end{itemize}
}

\section{Stochastic chemical kinetic models}
\subsection{Terminology}
\frame{\frametitle{}
	\setkeys{Gin}{width=0.8\textwidth}
	Imagine a \emph{well-mixed} space in \emph{thermal equilibrium} with
	\begin{itemize}[<+->]
	\item $M$ states: $S_1,\ldots,S_M$ with
	\item number of individuals $X_1,\ldots,X_M$ with elements $X_m\in\mathbb{Z}^+$
	\item which change according to $K$ transitions: $R_1,\ldots,R_K$ with
	\item propensities $a_1(x),\ldots,a_K(x)$.  
	\item The propensities are given by $a_k(x) = \lambda_k f_k(x)$ 
	\item where $f_k(x)$ is a known function of the system state. 
	\item If transition $k$ occurs, the state is updated by the stoichiometry $v_k$ with 
	\item elements $v_{ij}\in\{-2,-1,0,1,2\}$. 
	\end{itemize}
	
	\begin{columns}
	\hspace{0.15in}
	\begin{column}{0.8\textwidth}
	\includegraphics{system}
	\end{column}
	\hspace{-.6in}
	\begin{column}{0.4\textwidth}
	\uncover<3->{\includegraphics{rxns}}
	\end{column}
	\end{columns}
}

\subsection{$\tau$-leaping}
\frame{\frametitle{$\tau$-leaping}
	\setkeys{Gin}{width=0.6\textwidth}
	\begin{itemize}[<+->]
	\item If transition $k\in\{1,\ldots,K\}$ has the following probability
	\[ \lim_{\tau\to 0} \frac{P(\mbox{transition $k$ within the interval $(t,t+\tau)$}|X_t)}{\tau} = \lambda_{k} f_k(X_{t}),  \]
	\item[] then this defines a \alert{continuous-time Markov jump process}.
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	This model can be discretized using the $\tau$-leaping approximation:
	\[ \Delta X_{tk} \stackrel{ind}{\sim} Po(\lambda_{k} f_k(X_{t}) \tau) \]
	\pause and updating
	\[ X_{t+\tau,m} = X_{tm} + \sum_{k=1}^K v_{mk} \Delta X_{tk} \]
	\pause For simplicity, we'll set $\tau=1$, the observation interval.
}

\frame{\frametitle{Binomial-Poisson discrete-time state-space model}

	\vspace{-0.1in}

	\[ \begin{array}{rll}\label{def:Y}
	Y_{tk} &\stackrel{ind}{\sim} Bin(\Delta X_{tk},\theta_{k}), & k=1,\ldots,K \pause \\
	\Delta X_{tk} &\stackrel{ind}{\sim} Po(\lambda_{k} f_k(X_{t-1})), \\
	X_{tm} &= X_{t-1,m} + \sum_{k=1}^K v_{mk} \Delta X_{tk}, & m=1,\ldots,M 
	\end{array} \]
	
	\vspace{0.2in} \pause
	
	{\tiny
	\begin{center}
	\begin{tikzpicture}
\tikzstyle{main}=[circle, minimum size = 8mm, thick, draw =black!80, node distance = 8mm]
\tikzstyle{empty}=[circle,minimum size = 0mm, thick, draw =white, node distance = 8mm]
\tikzstyle{connect}=[-latex, thick]
\tikzstyle{box}=[rectangle, draw=black!100]
  \node[empty] (Xtm2)  { $\ldots$ }; 
  \node[main] (Xtm1) [right=of Xtm2] { $X_{t-1}$ }; 
  \node[main] (Xt) [right=of Xtm1] { $X_{t\phantom{+1}}$ }; 
  \node[main] (Xtp1) [right=of Xt] { $X_{t+1}$ }; 
  \node[empty] (Xtp2) [right= of Xtp1] { $\ldots$ }; 
  \path (Xtm2) edge [connect] (Xtm1)
           (Xtm1) edge [connect] (Xt)
           (Xt) edge [connect] (Xtp1)
           (Xtp1) edge [connect] (Xtp2); 
           
\alt<4->{         
  \coordinate (mid1) at ($(Xtm1.north)!0.5!(Xt.north)$);  
  \coordinate (mid2) at ($(Xt.north)!0.5!(Xtp1.north)$);  
  \coordinate (mid3) at ($(Xtp1.north)!0.5!(Xtp2.north)$);  
  \node[main] (Ytm1) [above=of mid1] { $Y_{t-1}$ }; 
  \node[main] (Yt) [above=of mid2] { $Y_{t\phantom{+1}}$ }; 
  \node[main] (Ytp1) [above=of mid3] { $Y_{t+1}$ }; 
  
  \path (mid1) edge [connect] (Ytm1)
           (mid2) edge [connect] (Yt)
           (mid3) edge [connect] (Ytp1);  
}      {           
  \node[main] (Ytm1) [above=of Xtm1] { $Y_{t-1}$ }; 
  \node[main] (Yt) [above=of Xt] { $Y_{t\phantom{+1}}$ }; 
  \node[main] (Ytp1) [above=of Xtp1] { $Y_{t+1}$ }; 
  \path (Xtm1) edge [connect] (Ytm1)
           (Xt) edge [connect] (Yt)
           (Xtp1) edge [connect] (Ytp1); 
 }             
\end{tikzpicture}
\end{center}
}

}



\subsection{SIR model}
\frame{\frametitle{S$\to$I$\to$R stochastic compartment model}\small
	An SIR compartment model tracks the number of 
	\begin{itemize}
	\item Susceptibles (S)
	\item Infectious (I)
	\item Recovered (R) 
	\end{itemize}
	usually with the stipulation that $N=S+I+R$ is constant.

	\vspace{0.2in} \pause 

	A stochastic SIR model has $M=3$ (states) and $K=2$ (transitions) \pause with $X_t = (S_t,I_t,R_t)$,  \pause 
\[	v = \bordermatrix{
	& S\to I & I\to R \cr
	S &            -1 &  \phantom{-}0 \cr
	I &  \phantom{-}1 &            -1 \cr
	R &  \phantom{-}0 &  \phantom{-}1
	}, \]
\pause
$f_1(X_t) = S_t I_t / N$, and $f_2 (X_t) = I_t$. 
}

\frame{\frametitle{S$\to$I$\to$R stochastic compartment model}
\setkeys{Gin}{width=0.6\textwidth}

\vspace{-0.2in}

	\[ \Delta X_{\mbox{S}\to\mbox{I}}\sim Po(\lambda_{\mbox{S}\to\mbox{I}} \mbox{S}\mbox{I} / \mbox{N} ) \qquad \Delta X_{\mbox{I}\to\mbox{R}} \sim Po(\lambda_{\mbox{I}\to\mbox{R}} \mbox{I}) \]
	
	\pause
	
	\begin{center}
	\includegraphics{simulation}
	\end{center}
}

\frame{\frametitle{Binomial sampling of transitions}
\setkeys{Gin}{width=0.6\textwidth}

\vspace{-0.2in}
	\[ Y_{\mbox{S}\to\mbox{I}} \sim Bin(\Delta X_{\mbox{S}\to\mbox{I}}, \theta_{\mbox{S}\to\mbox{I}}) \qquad Y_{\mbox{I}\to\mbox{R}} \sim Bin(\Delta X_{\mbox{I}\to\mbox{R}}, \theta_{\mbox{I}\to\mbox{R}}) \]
	
		\pause
	
	\begin{center}
	\includegraphics{data}
	\end{center}
}

\frame{\frametitle{Binomial sampling of transitions}
\setkeys{Gin}{width=0.6\textwidth}

\vspace{-0.2in}
	\[ Y_{\mbox{S}\to\mbox{I}} \sim Bin(\Delta X_{\mbox{S}\to\mbox{I}}, \theta_{\mbox{S}\to\mbox{I}}) \qquad Y_{\mbox{I}\to\mbox{R}} \sim Bin(\Delta X_{\mbox{I}\to\mbox{R}}, \theta_{\mbox{I}\to\mbox{R}}) \]
	
	\begin{center}
	\includegraphics{cumdata}
	\end{center}
}

\frame{\frametitle{Outline}
	\begin{itemize}
	\item Measles outbreak in Zimbabwe
	\item Model for low counts in disease outbreaks
	\item  \alert{Particle learning}
	\item Simulation study
	\item Application to outbreak in Zimbabwe
	\end{itemize}
}


\frame{\frametitle{Bayesian inference}

	\vspace{-0.1in}

	\[ \begin{array}{rll}\label{def:Y}
	Y_{tk} &\stackrel{ind}{\sim} Bin(\Delta X_{tk},\theta_{k}), & k=1,\ldots,K \\
	\Delta X_{tk} &\stackrel{ind}{\sim} Po(\lambda_{k} f_k(X_{t-1})), \\
	X_{tm} &= X_{t-1,m} + \sum_{k=1}^K v_{mk} \Delta X_{tk}, & m=1,\ldots,M \\
	\pause \\
	\theta_k &\stackrel{ind}{\sim} Be(a_{0k},b_{0k}), \pause \\
	\lambda_k &\stackrel{ind}{\sim} Ga(c_{0k}, d_{0k}), \pause \\
	X_0 &\sim Mult(N;\chi_1,\ldots,\chi_M) 
	\end{array} \]
	
	\vspace{0.2in} \pause
	
	Filtered distribution:
	\[ 
	p(X_t,\lambda,\theta|y_{1:t})
	\]
	\pause
	Forecast distribution
	{\small
	\[ 
	p(X_{t+1:T},y_{t+1:T}|y_{1:t}) \pause = \int \int \int p(X_{t+1:T},y_{t+1:T}|X_t,\lambda,\theta) p(X_t,\lambda,\theta|y_{1:t}) d\lambda d\theta dX_t
	\]}
	
}


\section{Particle learning}
\frame{\frametitle{Particle learning}
	Approximating a filtered distribution:
	\[ \uncover<1->{p(X_t,\lambda,\theta|y_{1:t})} 
	   \uncover<2->{\approx J^{-1} \sum_{j=1}^J \delta_{(X_t,\alert<6->{\psi})^{(j)}}}
	   \uncover<5->{\alert{p(\lambda|\psi^{(j)})p(\theta|\psi^{(j)})}}  \]
	 \uncover<3->{where} 
	\begin{itemize}
	\item<3-> $\delta_{(X_t,\alert<6->{\psi})^{(j)}}$ indicates a \emph{particle} location
	\item<4-> \alert{$\psi$ are particle sufficient statistics}
	\item<5-> \alert{$p(\lambda|\psi^{(j)})$ is a joint distribution for all rate parameters}
	\item<5-> \alert{$p(\theta|\psi^{(j)})$ is a joint distribution for all sampling parameters}
	\end{itemize}
	
	\vspace{0.2in}
	
	\uncover<6->{Intuition: }
	\begin{itemize}
	\item<6-> each particle represents a current belief about the world
	\item<7-> lots of particles provide uncertainty about this belief
	\end{itemize}
}

\frame{\frametitle{Particle learning: going from $t$ to $t+1$}
	Start with $p(X_t,\lambda,\theta|y_{1:t}) \approx J^{-1} \sum_{j=1}^J \delta_{(X_t,\psi_t)^{(j)}}p(\lambda|\psi_t^{(j)})p(\theta|\psi_t^{(j)})$ 
	
	\vspace{0.2in} \pause
	
	\begin{enumerate}[<+->]
	\item For all particles, 
		\begin{enumerate}
		\item Sample $\theta^{(j)}\sim p(\theta|\psi^{(j)})$.
		\item Calculate $w_j \propto p(y_{t+1}|X_t^{(j)}, \theta^{(j)}, \psi^{(j)})$.
		\end{enumerate}
	\item For $j=1,\ldots,J$
		\begin{enumerate}
		\item Sample $j^*$ with probability $w_{j^*}$.
		\item Sample $\lambda^{(j)} \sim p(\lambda|\psi^{(j^*)})$
		\item Sample $\Delta X_{t+1}^{(j)}\sim p(\Delta X|\lambda^{(j)}, \theta^{(j^*)},X_t^{(j^*)}, y_{t+1})$. 
		\item Update $X_{t+1}^{(j)}$ based on $X_t^{(j^*)}$ and $\Delta X_{t+1}^{(j)}$.
		\item Update $\psi_{t+1}^{(j)} = \mathcal{S}(\psi_t^{(j^*)}, y_{t+1}, \Delta X_{t+1}^{(j)})$.
		\end{enumerate}
	\end{enumerate} 
	
	\vspace{0.2in} \pause 
	
	End with $p(X_{\alert{t+1}},\lambda,\theta|y_{1:\alert{t+1}}) \approx J^{-1} \sum_{j=1}^J \delta_{(X_{\alert{t+1}},\psi_{\alert{t+1}})^{(j)}}p(\lambda|\psi_{\alert{t+1}}^{(j)})p(\theta|\psi_{\alert{t+1}}^{(j)})$
}

\subsection{Particle sufficient statistics}
\frame{\frametitle{Particle sufficient statistics}

\vspace{-0.2in}

{\tiny ($k$ subscript is implicit on the next 3 slides)}

\vspace{0.2in}

	Recall the model
		\[ \begin{array}{rl@{\qquad}rl}
	Y_{t+1} & \sim Bin(\Delta X_{t+1},\theta) & \Delta X_{t+1} & \sim Po(\lambda_{t} f(X_{t})), \\
	\theta|y_{1:t} & \sim Be(a_{t},b_{t}), & \lambda|y_{1:t} & \sim Ga(c_{t}, d_{t})
	\end{array} \]
	
  \pause
	
	Set $\psi_t = (a_t,b_t,c_t,d_t)$, \pause then 
	\[ \begin{array}{rl}
a_{t+1} &= a_{t} +y_{t+1},  \\
b_{t+1} &= b_{t} +\Delta X_{t+1} - y_{t+1}, \pause \\
c_{t+1} &= c_{t}+\Delta X_{t+1} , \\ % was + y_{t}
d_{t+1} &= d_{t} + f(X_{t}). \label{eqn:omegaupdate}
\end{array} \]
	\pause This defines $\psi_{t+1} = \mathcal{S}(\psi_{t}, y_{t+1}, \Delta X_{t+1})$.
}

\subsection{Conditional forward propagation}
\frame{\frametitle{Conditional forward propagation}
	Recall the model
	\[ \begin{array}{rll}
	Y_{t+1} & \sim Bin(\Delta X_{t+1},\theta_{}) \\
	\Delta X_{t+1} & \sim Po(\lambda f(X_{t}))
	\end{array} \]
	
	\vspace{0.2in} \pause 
	
	Then $p(\Delta X_{t+1}|\lambda, \theta,X_t, y_{t+1})$ is 
	\[ \begin{array}{rl}
	\Delta X_{t+1} &= y_{t+1} + Z_{t+1} \\
	Z_{t+1} & \sim Po([1-\theta]\lambda f(X_{t}))
	\end{array} \]
	\pause 
	by an appeal to Bayes' Rule, a change of variables, and the marginal distribution for $Y_t$:
	\[ Y_{t+1}  \sim Po(\theta\lambda f(X_{t})).\]
	 
}

\subsection{One-step ahead predictive distribution}
\frame{\frametitle{One-step ahead predictive distribution}
	From the previous slide and model construction:
	\[ \begin{array}{rll}
	Y_{t+1} & \sim Po(\theta \lambda f(X_{t})) \\
	\lambda|y_{1:t} & \sim Ga(c_{t}, d_{t}) 
	\end{array} \]
	
	\pause 
	
	Then 
	\[ Y_{t+1} | \theta_, \psi_{t}, X_t \sim NegBin( c_{t}, e_{t}) \]
	\pause where 
	\[ e_{t} = \frac{\theta f(X_t)}{d_{t} + \theta f(X_t)}.  \]
	\pause and 
	\begin{itemize}
	\item $c_{t}$ is the number of failures, 
	\item $Y_{t+1}$ is the number of successes, and 
	\item $e_{t}$ is the success probability.
	\end{itemize}
}
	
\frame{\frametitle{Outline}
	\begin{itemize}
	\item Measles outbreak in Zimbabwe
	\item Model for low counts in disease outbreaks
	\item Particle learning
	\item \alert{Simulation study}
	\item Application to outbreak in Zimbabwe
	\end{itemize}
}

	
\section{Simulation}
\frame{\frametitle{Simulation study}
	\begin{itemize}[<+->]
	\item 100 simulations from
		\begin{itemize}
		\item $X_0 \sim Mult(16100; (.994, .006, 0))$ corresponds to $E[I_0]=100$
		\item $\theta_{S\to I},\theta_{I\to R} \sim Be(50,950)$ for all $k$
		\item $\lambda_{S\to I} \sim Ga(50,100)$
		\item $\lambda_{I\to R} \sim Ga(25,100)$
		\item Ensured simulations had at least one $S\to I$ observation in first 5 time points
		\end{itemize}
	\item Settings
		\begin{itemize}
		\item 500 particles
		\item multinomial resampling
		\end{itemize}
	\end{itemize}
}

\subsection{Example filtered credible intervals}
\frame{\frametitle{}
\setkeys{Gin}{width=0.7\textwidth}
	\begin{center}
	\includegraphics[page=23]{example-plots}
	\end{center}
}

\frame{\frametitle{}
\setkeys{Gin}{width=0.7\textwidth}
	\begin{center}
	\includegraphics[trim=0.5cm 6.6cm 12cm 6cm, clip=true, page=23]{example-plots}
	\end{center}
}

\frame{\frametitle{}
\setkeys{Gin}{width=0.7\textwidth}
	\begin{center}
	\includegraphics[trim=6.5cm 0.7cm 6cm 12cm, clip=true, page=23]{example-plots}
	\end{center}
}

\frame{\frametitle{}
\setkeys{Gin}{width=0.7\textwidth}
	\begin{center}
	\includegraphics[trim=0.7cm 12.5cm 12.5cm 0.3cm, clip=true, page=23]{example-plots}
	\end{center}
}

\frame{\frametitle{Coverage}
\setkeys{Gin}{width=0.6\textwidth}
	\begin{center}
	\includegraphics{coverage}
	\end{center}
}

\frame{\frametitle{RMSE}
\setkeys{Gin}{width=0.6\textwidth}
	\begin{center}
	\includegraphics{mse}
	\end{center}
}

\frame{\frametitle{Outline}
	\begin{itemize}
	\item Measles outbreak in Zimbabwe
	\item Model for low counts in disease outbreaks
	\item Particle learning
	\item Simulation study
	\item \alert{Application to outbreak in Harare}
	\end{itemize}
}


\section{Harare measles outbreak}
\frame{\frametitle{Harare measles outbreak}
	\begin{itemize}[<+->]
	\item Model
		\begin{itemize}
		\item Known incubation period: $S\to E\to I\to R$
		\item Only observe weekly $E\to I$ transitions
		\end{itemize}
	\item Priors
		\begin{itemize}
		\item $N\sim Bin(1.5M, 0.01)$ 
		\item $X\sim Mult(N, (.998, .001,.001,0))$
		\item $\theta_{S\to E} = \theta_{I\to R} = 0$
		\item $\theta_{E\to I} \sim Be(10, 990)$
		\item $\lambda_{S\to E} \sim Ga(1,1)$
		\item {\color{blue}  $\lambda_{E\to I}=1$} and {\color{red}  $\lambda_{E\to I} \sim Ga(1,1)$ }
		\item $\lambda_{I\to R} \sim Ga(1,1)$
		\end{itemize}
	\item Settings
		\begin{itemize}
		\item 10,000 particles
		\item stratified resampling
		\end{itemize}
	\end{itemize}
}
	
\frame{\frametitle{Harare measles outbreak}
\setkeys{Gin}{width=0.9\textwidth}
	\begin{center}
	\includegraphics{harare-fit}
	\end{center}
}
	
\subsection{Summary}
\frame{\frametitle{Summary}
	\begin{itemize}[<+->]
	\item discrete-time binomial-Poisson state-space model
	\item Particle learning plus integrate of states
	\item Computationally efficient
	\item Data - timely, accurate, disaggregated, usable, \pause e.g.
	\url{https://github.com/rambaut/MERS-Cases/blob/gh-pages/data/cases.csv}
	\item Slides: \url{https://github.com/jarad/IDM2016}
	\item {\tt tlpl} R package: \url{https://github.com/jarad/IDM2016}
	\end{itemize}
	
	\vspace{0.2in} \pause
	
	\begin{center}
	{\Huge Thank you!}
	\end{center}
}

\section{Theoretical guarantees}
\frame{\frametitle{Theoretical results}
	 Specifically, from Section 3.5.1 of Del Moral 2004, for bounded functions $f_t$ and any $p>1$, the following result holds
\[ E_{e_0}^J \left[ \left| e_t^J(f_t)-e_t(f_t)\right|^p\right]^{1/p} \le \frac{a(p) b(t) ||f||}{\sqrt{J}} \]
where 
\begin{itemize}
\item $e_t(f_t)$ is the expectation of $f_t$ under the true filtered distribution at time $t$,
\item $e_t^J(f_t)$ is the expectation of $f_t$ under the particle approximation at time $t$ using $J$ particles,
\item $a(p)$ is a function of $p$, 
\item $b(t)$ is an increasing function of $t$ that depends on which algorithm is used, and 
\item $||\cdot||$ is the supremum norm.
\end{itemize}
}


	
\end{document}

